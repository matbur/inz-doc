%\documentclass{report}
%
%\usepackage{listings}
%\usepackage{polski}
%\usepackage[utf8]{inputenc}
%\usepackage[margin=1in]{geometry}
%\usepackage[english,polish]{babel}\usepackage{indentfirst}
%\usepackage[T1]{fontenc}
%\usepackage{xcolor}
%\usepackage{graphicx}
%\usepackage{float}
%\usepackage{longtable}
%
%% Default fixed font does not support bold face
%\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
%\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal
%
%% Custom colors
%\usepackage{color}
%\definecolor{deepblue}{rgb}{0,0,0.5}
%\definecolor{deepred}{rgb}{0.6,0,0}
%\definecolor{deepgreen}{rgb}{0,0.5,0}
%\newcommand\pythonstyle{\lstset{
%language=Python,
%basicstyle=\ttm,
%otherkeywords={self},             % Add keywords here
%keywordstyle=\ttb\color{deepblue},
%emph={MyClass,__init__},          % Custom highlighting
%emphstyle=\ttb\color{deepred},    % Custom highlighting style
%stringstyle=\color{deepgreen},
%frame=tb,                         % Any extra options here
%showstringspaces=false            %
%}}
%
%% Python environment
%\lstnewenvironment{python}[1][]
%{
%\pythonstyle
%\lstset{#1}
%}
%{}
%
%% Python for external files
%\newcommand\pythonexternal[2][]{{
%\pythonstyle
%\lstinputlisting[#1]{#2}}}
%
%% Python for inline
%\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}

%\pagecolor{black}
%\color{white}

\include{settings}

\begin{document}
    \maketitle
    \tableofcontents

    \chapter{Wstęp}\label{ch:wstęp}

    \section{Cel projektu}\label{sec:celProjektu}

    Celem pracy jets zastosowanie sztucznych sieci neuronowych jako klasyfikatora w zadaniu diagnozowania stanów ostrego brzucha, ich komputerowa implementacja oraz przeprowadzenie badań eksperymentalnych na danych rzeczywistych w celu oceny skuteczności algorytmów.

    \section{Zadania do wykonania}\label{sec:zadaniaDoWykonania}

    \begin{enumerate}
        \item Zapoznanie się z medycznym problemem diagnostycznym i danymi empirycznymi - sformułowanie problemu rozpoznawania (klasy, cechy),
        \item Liczbowa ocena przydatności poszczególnych cech z zastosowaniem wybranej selekcji typu filtr - określenie rankingu cech,
        \item Określenie typu i architektury sieci neuronowej jako klasyfikatora stanów ostrego brzucha - komputerowa implementacja klasyfikatora,
        \item Zaplanowanie i przeprowadzenie badań eksperymentalnych - analiza wyników i sformułowanie wniosków,
        \item Redakcja pracy dyplomowej.
    \end{enumerate}

    \chapter{Problem medyczny}\label{ch:problemMedyczny}

    Wybrany przeze mnie problem medyczny dotyczy klasyfikacji stanów ostrego brzucha.
    Jest to szereg objawów charakterystycznych dla różnych schorzeń internistycznych i chirurgicznych.
    Za ten stan odpowiedzialne mogą być różne choroby.
    Ostry brzuch jest zagrożeniem dla życia, dlatego zawsze wymaga interwencji lekarza~\cite{ostry-brzuch}.

    \section{Opis chorób}\label{sec:opisChorób}

    Do klasyfikacji jest 8 chorób, zatem sieć neuronowa będzie miała za zadanie przypisać 1~z~8 klas.
    Są to:
    \begin{enumerate}
        \item Ostre zapalenie wyrostka robaczkowego,
        \item Zapalenie uchyłków jelit,
        \item Niedrożność mechaniczna jelit,
        \item Perforowany wrzód trawienny,
        \item Zapalenie woreczka żółciowego,
        \item Ostre zapalenie trzustki,
        \item Niecharakterystyczny ból brzucha,
        \item Inne przyczyny ostrego bólu brzucha.
    \end{enumerate}

    Trzy pierwsze choroby to ostre schorzenia jelit.
    Kolejne trzy są ostrymi schorzeniami organów trawiennych.
    Pozostałe to inne stany „ostrego brzucha”.

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.7]{./img/histogram.png}
        \caption{Histogram występowania chorób}
        \label{hist}
    \end{figure}

    Histogram~\ref{hist} pokazuje, że rozkład klas jest nierównomierny.
    Na 476 obiektów aż 157 to „Niecharakterystyczny ból brzucha” i 141 ma etykietę „Ostre zapalenie wyrostka robaczkowego”.
    Czyli do 2 klas należy ponad 60\% obiektów.
    Może to mieć negatywny wpływ na jakość klasyfikacji.

    \section{Opis cech}\label{sec:opisCech}

    Dane do tego problemu zawierają 31 cech.
    Są to odpowiedzi z wywiadu medycznego i~wyniki przeprowadzonych badań.
    Możliwe wartości parametrów przedstawione są poniżej.
    Jak widać wszystkie liczby są naturalne mniejsze niż 11, tak że normalizacja czy skalowanie danych nie jest konieczne.

    \input{data/questions}

    \section{Selekcja cech}\label{sec:selekcjaCech}

    \subsection{Motywacja}\label{subsec:motywacja}

    Selekcja cech jest procesem wymaganym, gdy dane nie są dobrej jakości w wielu algorytmach uczenia maszynowego.
    Polega ona na wyborze podzbioru najlepszych cech według ustalonego kryterium.
    Analitycy danych przeprowadzają selekcję z następujących powodów:
    \begin{itemize}
        \item uproszczenie modelu, w celu ułatwienia interpretacji przez badaczy,
        \item skrócenie czasu treningu,
        \item zmniejszenie wymiarowości modelu,
        \item zwiększenie generalizacji poprzez uniknięcie zjawiska przeuczenia.
    \end{itemize}

    Nie wszystkie cechy nadają się do procesu klasyfikacji, dlatego konieczne jest przeprowadzenie selekcji cech.

    \subsection{Test chi2}\label{subsec:testChi2}

    Metoda, którą wybrano to test chi2~\cite{chi2}.
    Jest to jedna z technik nieparametrycznych.
    Nadaje się bardzo dobrze do oceny istotności statystycznej cechy.
    Test ten polega na obliczeniu podanego poniżej wyrażenia dla każdej z cech i wybraniu takich, dla których wartość jest największa.
    Według tego wzoru obliczana jest wartość chi2:

    \begin{equation}
        \chi ^ 2 = \sum_{i=1}^{n}{ \frac{{(O_i - E_i) ^ 2}}{E_i}},
    \end{equation}

    gdzie:
    \begin{itemize}
        \item $O_i$ - wartość mierzona,
        \item $E_i$ - wartość oczekiwana,
        \item $n$ - liczba obiektów.
    \end{itemize}

    Wartości testu dla wszystkich cech mają następujące wartości:

    \vspace{1em}

    \begin{longtable}{|c|l|l|}
        \caption{Wartości chi2 dla wszystkich cech}\\ \hline
        \textbf{L.p.} & \textbf{Cecha} & \textbf{Wartość chi2} \\ \hline
        \endfirsthead
        \multicolumn{3}{c}
        {\tablename\ \thetable\ -- \textit{Wartości chi2 dla wszystkich cech - c.d.}} \\ \hline
        \textbf{L.p.} & \textbf{Cecha} & \textbf{Wartość chi2} \\ \hline
        \endhead
        \hline \multicolumn{3}{r}{\textit{Kontynuacja na następnej stronie}} \\
        \endfoot
        \hline
        \endlastfoot
        1 & Charakter bólu obecnie & 127.811 \\
        \hline 2 & Czynniki przynoszące ulgę & 87.453 \\
        \hline 3 & Nudności i wymioty & 84.633 \\
        \hline 4 & Czas trwania bólu & 84.273 \\
        \hline 5 & Umiejscowienie bolesności uciskowej & 77.456 \\
        \hline 6 & Lokalizacja bólu obecnie & 70.865 \\
        \hline 7 & Czynniki nasilające ból & 59.357 \\
        \hline 8 & Tętno & 58.152 \\
        \hline 9 & Apetyt & 54.489 \\
        \hline 10 & Wypróżnienia & 42.184 \\
        \hline 11 & Charakter bólu na początku zachorowania & 32.127 \\
        \hline 12 & Lokalizacja bólu na początku zachorowania & 31.430 \\
        \hline 13 & Ruchy oddechowe powłok brzusznych & 31.192 \\
        \hline 14 & Progresja bólu & 30.502 \\
        \hline 15 & Objaw Blumberga & 21.387 \\
        \hline 16 & Wiek & 21.228 \\
        \hline 17 & Skóra & 20.202 \\
        \hline 18 & Intensywność bólu & 18.438 \\
        \hline 19 & Temperatura (pacha) & 17.708 \\
        \hline 20 & Stan psychiczny & 15.930 \\
        \hline 21 & Leki & 15.554 \\
        \hline 22 & Objaw Murphy'ego & 13.666 \\
        \hline 23 & Obrona mięśniowa & 13.062 \\
        \hline 24 & Oddawanie moczu & 12.322 \\
        \hline 25 & Wzmożone napięcie powłok brzusznych & 11.406 \\
        \hline 26 & Wzdęcia & 8.771 \\
        \hline 27 & Opory patologiczne & 8.504 \\
        \hline 28 & Poprzednie operacje brzuszne & 7.007 \\
        \hline 29 & Płeć & 6.195 \\
        \hline 30 & Poprzednie niestrawności & 4.470 \\
        \hline 31 & Żółtaczka w przeszłości & 0.590 \\
    \end{longtable}

    \vspace{1em}

    Najlepszymi cechami są te, które mają wysoką wartość chi2.
    Zatem ograniczając liczbę cech, do klasyfikacji brane będą te z góry tabeli.
    Cechy o niskiej wartości, jak na przykład „Żółtaczka w przeszłości”, nie polepszą klasyfikacji, a mogą ją nawet pogorszyć.

    \chapter{Techologie}\label{ch:techologie}

    \section{Python}\label{sec:python}

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.4]{./img/python-logo.png}
        \caption{Logo języka Python}
        %        \label{}
    \end{figure}

    Python to otwarto-źródłowy, wysoko poziomowy język programowania ogólnego przeznaczenia.
    Stworzony został 26 lat temu przez holenderskiego programistę Guido van Rossuma.
    Najpolularniejszy interpreter Pythona napisany jest w języku C.
    Jednak w odróżnieniu od C, C++ i Javy, Python jest interpretowalny i nie używa się w nim nawiasów klamrowych do oddzielenia bloków kodu.
    Jest przez to bardziej czytelny i nie odstrasza ludzi aspirujących do bycia programistami.
    Zamias klamr stosuje się wcięcia w kodzie, które powinny wynosić 4 spacje na każdy poziom.
    Od wersji 3.5 w Pythonie można jawnie stosować typowanie, czyli na przykład twórca funkcji może umieścić informację w~kodzie, jakiego typu powinny być argumenty i jaki typ funkcja zwraca.
    Dzięki temu czas potrzebny na zrozumienie cudzego kodu staje się krótszy.

    Python ma wiele zastosowań:

    \begin{itemize}
        \item nauka programowania,
        \item web development,
        \item aplikacje konsolowe,
        \item aplikacje okienkowe,
        \item gry komputerowe,
        \item naukowe,
        \item analiza danych.
    \end{itemize}

    Od kilku lat Python zyskuje duże zainteresowanie naukowców z różnych dziedzin nauki z racji swojej prostoty i wszechstronności.
    Powstało również wiele gotowych modułów do zastosowania w uczeniu maszynowym, ale nie będę używał ich w tym projekcie.

    \section{NumPy}\label{sec:numpy}

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.4]{./img/numpy-logo.png}
        \caption{Logo NumPy}
        %        \label{}
    \end{figure}

    NumPy to otwarto-źródłowa biblioteka do Pythona służąca do obliczeń naukowych.
    Umożliwia przechowywanie danych w wielowymiarowych tablicach i macierzach (tensorach) oraz wykonywanie skomplikowanych funkcji na nich.
    Napisana została w większości w języku C, co sprawia, że kod wykonywany jest szybciej niż w samym Pythonie.
    Tablice z NumPy są wykorzystywane w wielu bibliotekach, jako podstawowa struktura danych.
    W tym projekcie użyto jej do przechowywania wag w każdej warstwie sieci.

    \section{matplotlib}\label{sec:matplotlib}

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.4]{./img/matplotlib-logo.png}
        \caption{Logo matplotlib}
        %        \label{}
    \end{figure}

    Matplotlib to najpopularniejsza biblioteka do tworzenia wykresów w Pythonie.
    Wraz z biblioteką NumPy bardzo często wykorzystywana jest do analizy i wizualizacji danych.
    Jest bardzo prosta w obsłudze.
    Pozwala również wyświetlać obrazy w oknach z poziomu skryptu w Pythonie.
    W kilka linii jesteśmy w stanie stworzyć prosty wykres i wyeksportować go do pliku graficznego.
    Wspiera takie typy wykresów jak:

    \begin{itemize}
        \item liniowy,
        \item histogram,
        \item punktowy,
        \item 3D,
        \item biegunowy.
    \end{itemize}

    Biblioteki tej użyto do wygenerowania wszyskich wykresów w tej pracy.

    \section{pandas}\label{sec:pandas}

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.4]{./img/pandas-logo.png}
        \caption{Logo pandas}
        %        \label{}
    \end{figure}

    Pandas to biblioteka napisana w Pythonie służąca do manipulacji i analizy danych.
    Oferuje struktury danych, które ułatwiają operowanie na plikach csv, json i xlsx.
    Umożliwia operacje podobne do znanych z języka SQL.
    Są to: grupowanie danych, sortowanie po indeksie lub po innej kolumnie, łączenie tabel i usuwanie duplikatów.

    W tym projekcie pandasy użyto do czytania plików csv i dostarczania danych do sieci neuronowej.

    \section{Git}\label{sec:git}

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{./img/git-logo.png}
        \caption{Logo Gita}
        %        \label{}
    \end{figure}

    Git to rozproszony system kontroli wersji, czyli narzędzie do śledzenia zmian w plikach źródłowych.
    Jest to oprogramowanie używane głównie do zarządzania kodem, ale może być używane również do trzymania historii innych plików.
    Git ma na celu szybkość, spójność danych i wspieranie pracy rozproszonej wśród zespołów.
    Nie wymaga ciągłego dostępu do Internetu.
    Jest wykorzystywany w prawie wszystkich nowoczesnych projektach.

    Git został napisany przez Linusa Torvaldsa w 2005 roku, jako narzędzie do tworzenia jądra Linuksa, gdyż żaden inny system kontroli wersji nie spełniał jego wymagań.

    Główną strukturą w Gicie jest repozytorium.
    Każde repozytorium przypisane jest do jednego projektu.
    Posiada ono historię w formie grafu skierowanego, który jest drzewem.
    Git umożliwia poruszanie się po tym drzewie pozwalając przeglądać repozytorium w danym stanie.

    Praca z Gitem rozpoczyna się od sklonowania istniejącego repozytorium lub stworzenia nowego, pustego.
    Użytkownik po zmianie jakiegoś pliku śledzonego przez Gita może zrobić „commit”, czyli zapisać obecny stan projektu.
    Każdy „commit” ma przypisaną wiadomość, w której twórca „commita” informuje, co zmienił.
    Po „scommitowaniu” można zsynchronizować stan repozytorium z głównym serwerem.
    Dopiero wtedy inni użytkownicy mogą zobaczyć, jakie zaszły zmiany i pobrać do siebie najnowszą wersję.

    Git to potężne narzędzie, każdy programista powinien potrafić z niego korzystać.
    Łatwo jest poznać podstawy Gita i nie wymaga dużo czasu opanowanie ich.
    Zaawansowana znajomość Gita pozwala na robienie niesamowitych rzeczy w repozytorium.

    W projekcie inżynierskim korzystano z Gita do zapisywania postępów w tworzeniu aplikacji.
    Kod jest przechowywany na serwerze firmy GitHub.

    \section{Docker}\label{sec:docker}

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.4]{./img/docker-logo.png}
        \caption{Logo Dockera}
        %        \label{}
    \end{figure}

    Docker to otwarto-źródłowe narzędzie służące do konteneryzacji aplikacji.
    Zapewnia dodatkową warstę abstrakcji nad systemem operacyjnym.
    Działa zarówno na Linuksie, jak i na Windowsie.
    Pierwsze wersje Dockera od 2013 roku napisane były w Pythonie, a~kolejne w języku Go.

    W wielu aplikacjach Docker używany jest w celu ułatwienia wdrożenie aplikacji na serwery produkcyjne.
    Jest przydatny również w czasie wytwarzania dla deweloperów, gdyż idealnie nadaje się na środowisko testowe.

    Docker udostępnia na swojej stronie internetowej wiele predefiniowanych obrazów z~zainstalowanymi aplikacjami, które są gotowe do użycia.
    Zalogowani użytkownicy mogą również publikować swoje własne obrazy zbudowane przez nich.
    Pozwala to dzielić się swoją pracą z całą społecznością.

    Praca z Dockerem polega na uruchomieniu kontenera z wybranego obrazu.
    Obraz dockerowy to tak jakby zapisany stan maszyny wirtualnej.
    Kontener jest konkretną uruchomioną instancją obrazu.

    Poza oficjalnymi obrazami, istnieje również możliwość tworzenia własnych obrazów do poszczególnych aplikacji.
    Polega to na stworzeniu pliku domyślnie o nazwie Dockerfile, gdzie podany jest obraz bazowy oraz lista komend do wykonania.
    Po zbudowaniu obrazu jedną komendą, można uruchomiać kontenery.

    W projekcie inżynierskim skorzystano z Dockera, badania przeprowadzałem na serwerze, gdzie nie ma zainstalowanych wymaganych zależności.

    \chapter{Sieć neuronowa}\label{ch:siećNeuronowa}

    \section{Wprowadzenie}\label{sec:wprowadzenie}

    Sztuczna sieć neuronowa to pewna struktura matematyczna, która może być zaimplementowana programowo lub sprzętowo~\cite{neuron}.
    Początkowo taki twórcy takich modeli inspirowali się zwierzęcym mózgiem, w którym połączone ze sobą neurony tworzą sieć.
    Taka sieć przetwarza sygnały wejściowe wykonując na nich pewne operacje.
    Sieci wykorzystywane są często do rozwiązywania problemów klasyfikacji, z racji na ich zdolność uczenia.
    Na przykład potrafią przetwarzać zdjęcia i opisywać, co się na nich znajduje.
    Przed skorzystaniem z sieci należy ją nauczyć, co sprowadza się do przekazywania na wejście sieci danych uczących razem z poprawną klasą, do której dane obiekty należą.

    \section{Neuron}\label{sec:neuron}

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.4]{./img/neuron.png}
        \caption{Schemat neuronu}
        %        \label{}
    \end{figure}

    Neuron stanowi podstawowy budulec sztucznej sieci neuronowej.
    Składa się z ustalonej liczby wejść, wraz z odpowiadającymi im wagami.
    Ponadto neuron zawiera nieliniową funkcję aktywacji oraz jedno wyjście.
    Jego zadanie to obliczenie iloczynu skalarnego wektora wejść z wektorem wag zgodnie ze wzorem:

    \begin{equation}
        s = \sum _{i=0}^{n} {w^{(i)} \cdot x^{(i)}} + w^{(0)} = w^T \cdot x + w^{(0)},
        \label{neuron-eq1}
    \end{equation}

    gdzie:
    \begin{itemize}
        \item $s$ - pobudzenie neuronu,
        \item $w$ - wektor wag,
        \item $x$ - wektor wejściowy,
        \item $w^{(i)}$ - i-ta wartość wektora wag,
        \item $x^{(i)}$ - i-ta wektor wejściowego,
        \item $w^{(0)}$ - bias.
    \end{itemize}

    Dodatkowo możemy przyjąć, że bias jest dodatkowym wejściem neuronu o wartości 1.
    Wtedy wzór na pobudzenie neuronu przyjmuje następującą postać:

    \begin{equation}
        s = \sum _{i=0}^{n} {w^{(i)} \cdot x^{(i)}} = w^T \cdot x,
        \label{neuron-eq2}
    \end{equation}

    gdzie $x^{(0)} = 1$, a reszta symboli jak w~\ref{neuron-eq1}.

    Następnie obliczona suma ważona poddawana jest funkcji aktywacji i przekazywana na wyjście neuronu.
    W procesie uczenia wagi w neuronie zmieniają się tak, by wyliczona wartość funkcji błędu była jak najmniejsza.

    \subsection{Funkcja aktywacji}\label{subsec:funkcjaAktywacji}

    Funkcja aktywacji to funkcja, która wykorzystywana jest w sztucznych sieciach neuronowych, a dokładniej w samym neuronie do zmiany wartości wyjścia według wzoru:

    \begin{equation}
        y = f(s) = f(w^T \cdot x),
    \end{equation}

    gdzie:
    \begin{itemize}
        \item $y$ - wektor wyjść neuronów,
        \item $f$ - funkcja aktywacji,
        \item $s$ - pobudzenie neuronu.
    \end{itemize}

    Ma to na celu sprawienie, że sieć jest w stanie lepiej się uczyć nawet przy małej liczby neuronów.
    W uczeniu maszynowym znanych jest wiele rodzajów takich funkcji.
    W tej pracy opisano tylko dwie, które użyto do budowy sieci neuronowej.

    \subsubsection{Sigmoid}

    Pierwszą opisywaną funkcją jest sigmoid, zwana też „sigmoidalną funkcją unipolarną”.
    Bardzo dobrze nadaje się jako funkcja aktywacji, gdyż jej dziedzina to cały zbiór liczb rzeczywistych.
    Ma tę cechę, że zbiór warości mieści się w zakresie (0, 1).
    Jest to również wada, że szybko się „nasyca”.
    Kolejnym minusem tej funkcji jest to, że wartości nie zcentralizowane wokół zera.
    Ponadto wykorzystuje funkcję ekpotencjalną, która jest kosztowna obliczeniowo.
    Jej wzór to~\cite{sigmoid}:

    \begin{equation}
        \sigma(x) = \frac {1}{1+e^{-x}}
    \end{equation}

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.8]{./img/sigmoid.png}
        \caption{Funkcja aktywacji - sigmoid}
        %        \label{}
    \end{figure}

    \subsubsection{Tangens hiperboliczny}
    Kolejna funkcja, która wykorzystywana jest w sieciach neuronowych to tangens hiperboliczny (tanh).
    W tym przypadku również dziedziną jest zbiór liczb rzeczywistych.
    Spłaszcza wyjście w zakresie (-1, 1).
    Podobnie jak sigmoid szybko się „nasyca”.
    W przeciwieństwie do poprzedniej funkcji jest scentralizowany wokół zera.
    Tanh również korzysta z funkcji ekpotencjalnej~\cite{cs231_neural_2}.
    Jej wzór to:
    \begin{equation}
        \tanh(x) = \frac {2}{1+e^{-2x}} - 1
    \end{equation}

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.8]{./img/tanh.png}
        \caption{Funkcja aktywacji - tanh}
        %        \label{}
    \end{figure}

    \section{Model wielowarstwowy}\label{sec:modelWielowarstwowy}

    Kiedy pojedyncze neurony mają te same sygnały wejściowe, tworzą wówczas tak zwaną warstwę w sieci neuronowej.
    Dlatego sieć neuronowa ma budowę warstwową.
    Zawsze występuje warstwa wejściowa i wyjściowa.
    Ponadto mogą występować warstwy ukryte.
    Ich ilość zależy od rozwiązywanego problemu.

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.9]{./img/mlp.jpg}
        \caption{Schemat sieci neuronowej}
        %        \label{}
    \end{figure}

    W warstwie wejściowej znajduje się tyle neuronów, ile jest badanych cech.
    W moim przypadku będzie mniej niż 31.
    Neurony w tej warstwie nie mają wag, lecz przekazują dalej dokładnie to, co otrzymały.

    Liczba neuronów w warstwie wyjściowej również nie jest przypadkowa.
    Warstwa ta składa się z takiej samej liczby neuronów, co liczba klas w zadanym problemie.
    Problem medyczny, na którym pracuję dotyczy klasyfikacji ośmio klasowej, dlatego będzie osiem neuronów wyjściowych.
    Każdy z nich będzie zwracał wartość przynależności do danej klasy.

    \subsection{Proces uczenia}\label{subsec:procesUczenia}

    Uczenie sieci neuronowej, w którym znamy poprawne klasy dla danych uczących nazywamy uczeniem nadzorowanym lub uczeniem z nauczycielem.
    Polega na porównaniu wyjścia sieci z wartością oczekiwaną i dostrajaniu wag w neuronach tak, by minimalizować pewną ustaloną funkcję błędu.

    Korekcja wag odbywa się w procesie zwanym propagacją wsteczną.
    Propagacja wsteczna korzysta z gradientowych metod optymalizacji, które są wydajne obliczeniowo i bardzo skuteczne do uczenia sieci.
    Propagacja wsteczna wymaga również funkcję błędu.
    W tym celu stosuje się tak zwany „błąd średnio-kwadratowy”, który jest obliczany następująco:

    \begin{equation}
        Q(w_k) = \sum_{k=1}^{N}{(d_k - y_k) ^ 2},
    \end{equation}

    gdzie:

    \begin{itemize}
        \item $Q$ - błąd średniokwadratowy,
        \item $w_k$ - wektor wag w k-tym neuronie,
        \item $N$ - liczba danych uczących,
        \item $d_k$ - poprawna wartość,
        \item $y_k$ - aktualna wartość.
    \end{itemize}

    Algorytm jest wieloetapowy.
    Pierwszym krokiem jest dostarczenie obiektu na wejście sieci i sprawdzenie jakie otrzymamy wyjście.
    Następnie rekurencyjnie zmniejszana jest wartość funkcji błędu od końca sieci zmieniając wagi w kolejnych warstwach.

    Tutaj korzysta się ze współczynnika uczenia.
    Na początku powinien mieć ustaloną wartość, a następnie wraz z kolejnymi epokami uczenia może się zmniejszać.

    Należy również wyznaczyć pochodne funkcji aktywacji.
    Pierwsza pochodna funkcji sigmoid to równanie~\ref{der-sigmoid}, a tanh to~\ref{der-tanh}.

    \begin{equation}
        \sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))
        \label{der-sigmoid}
    \end{equation}

    \begin{equation}
        \tanh'(x) = 1 - \tanh(x)^2
        \label{der-tanh}
    \end{equation}

    Można to porównać do stania nad doliną górską, gdzie chcemy dość do najniższego punktu.
    Najpierw należy ustalić kierunek marszu, by później kroczyć w tym kierunku.
    Na początku kroki mogą być ogromne, ale w miarę schodzenia, powinniśmy je zmniejszać, by nie przekroczyć minimum.

    \chapter{Opis architektury aplikacji}\label{ch:opisArchitekturyAplikacji}

    \section{Schemat warstwy}\label{sec:schematWarstwy}

    \pythonexternal[caption={Schemat klasy Layer}]{./data/layer.py}

    Powyższy fragment kodu przedstawia schemat klasy Layer.
    Jest to implementacja jednej warstwy w sieci neuronowej.
    Przypomina schemat struktury danych zwanej listą dwukierunkową, gdyż zawiera referencje do poprzedniej i następnej warstwy.
    Klasa zawiera w sobie tablicę, która jest składa się z wag połączeń do poprzedniej warstwy.

    Przy tworzeniu instancji należy podać krotkę liczba oznaczającą kształt warstwy.
    Dodatkowo można przekazać nazwę funkcji aktywacji, którą domyślnie jest to sigmoid.

    Zadanie funkcji „feedforward” to przyjęcie tablicy liczb z poprzedniej warstwy, obliczenie iloczynu skalarnego z aktualnymi wagami i poddanie wyjścia funkcją aktywacji.
    Następnie funkcja powinna rekurencyjnie wywołać samą siebie na następnej warstwie jeśli nie jest ostatnia w sieci.
    W przeciwnym przypadku zwraca wyliczone wyjście całej sieci.

    Funkcja „calc\_delta” wywoływana jest rekurencyjne, ale w przeciwnym kierunku.
    Oblicza ona różnicę pomiędzy spodziewanym wyjściem warstwy a aktualnym.
    Pozwoli to później skorygować wagi każdej warstwy.

    Następna funkcja to „calc\_gradient”.
    Jest równnież wywoływana rekurencyjnie zaczynając od końca sieci.
    Oblicza wartość gradientu na podstawie wyjścia warstwy oraz wartości delty.

    Ostatnią funkcją jest „update\_weights”.
    Jak jej nazwa wskazuje, to właśnie ona zmienia wartości wag w warstwach odejmując iloczyn obliczonego gradientu ze współczynnikiem uczenia.
    W miarę uczenia współczynnik uczenia może się zmieniać, dlatego uznałem, że to dobre miejsce na dostarczenie tego współczynnika warstwie sieci neuronowej.

    \section{Tworzenie architektury sieci}\label{sec:tworzenieArchitekturySieci}

    \pythonexternal[caption={Schemat tworzenia sieci}]{./data/net.py}

    Funkcja „input\_data” tworzy pierwszą warstwę sieci neuronowej.
    Jako argument przyjmuje krotkę z wymiarem danych, które będą przekazywane do sieci.
    Ustawia w instacji warstwy, flagę informującą, że jest to warstwa wejściowa i zwraca ją.

    Druga ważna funkcja to „fully\_connected”.
    Pierwszym argumentem, który należy jej podać to istniejąca już sieć neuronowa składająca się z połączonych rekurencyjnie warstw.
    „n\_units” to parametr, który jest liczbą neuronów w nowo utworzonej warstwie.
    Na koniec można podać, jaką funkcję aktywacji ma mieć ta warstwa.
    Gdy nie zostanie podana, to domyślnie będzie to „sigmoid”.
    Ta funkcja ustawia odpowiednie flagi w instancji warstwy i przypisuje wskaźniki do następników i poprzedników, jak w liście dwu kierunkowej.

    Kilka kolejnych linii kodu to proces tworzenia całej sieci neuronowej.
    Ta sieć przyjmuje 30 cech i może przypisać każdy obiekt do 8 klas.
    Pomiędzy wejściem a wyjściem znajdują się 3 warstwy ukryte o rozmiarach 24, 16 i 12 neuronów odpowiednio.
    Taka sieć jest gotowa przyjęcia danych i do nauki klasyfikacji.

    \section{Schemat modelu}\label{sec:schematModelu}

    \pythonexternal[caption={Schemat klasy Model}]{./data/model.py}

    Klasa Model jest nakładką na sieć neuronową.
    To z niej użytkownik aplikacji bezpośrednio korzysta.
    Model w inicjalizerze przyjmuje jeden argument - zaprojektowaną sieć neuronową.

    Funcja „fit” odpowiada za uczenienie sieci.
    Przyjmuje wiele argumentów w celu sprarametryzowania procesu.
    \begin{itemize}
        \item X\_inputs - tablica ze wszystkimi danymi uczącymi bez przypisanych im klas,
        \item Y\_targets - tablica zawierająca numery klas dla obiektów uczących,
        \item validation\_set - krotka zawierająca dane testowe,
        \item learning\_rate - krotka z 2 liczbami: współczynnikiem uczenia na początku i na końcu,
        \item n\_epoch - liczba określająca liczbę epoch,
        \item shuffle - flaga informująca o tym, czy dane uczące mają być pomieszane,
        \item train\_file - nazwa pliku w którym zapisywany jest postęp uczenia.
    \end{itemize}

    Metoda „fit” zbiera wszystkie możliwe dane z procesu uczenia, loguje je na konsolę i~zapisuje do pliku.
    Dzięki temu możliwe jest przeprowadzenie badań i porównanie efektywności uczenia dla różnych wartości parametrów.

    Funkcje „predict” i „predict\_label” pozwalają skasyfikować wiele obiektów jednocześnie.
    Przyjmują tylko tablicę z wartościami cech.
    Jeśli potrzebne jest znać wsparcia wszystkich dla klas, to należy skorzystać z „predict”, a jeśli wystarczy informacja o najbardziej prawdopodobnej klasie, to można skorzystać z „predict\_label”.

    Ostatnie ważne funkcje to „save” i „load”.
    Obie potrzebują na wejściu nazwę pliku.
    Funkcja „save” zapisuje aktualne wagi do tego pliku w formie JSONa, a „load” wczytuje je ustawia wagi we wszyskich warstwach.
    Ważne jest, aby pamiętać, że architektura sieci musi się zgadzać.

    \chapter{Przeprowadzone badania}\label{ch:przeprowadzoneBadania}

    Do badań nad wpływem różnych parametrów na jakość klasyfikacji stworzyłem 120 różnych modeli sieci neuronowej.
    Modele te uczyły się na serwerze, który posiadał 30 CPU o nieznanych parametrach, z których wykorzystałem tylko 24 CPU.
    Ponadto serwer miał do dyspozycji 100 GB pamięci RAM.
    Czas uczenia wyniósł około 68 minut.

    Dodatkowo przeprowadziłem te same badania na innym serwerze posiadającym tylko 1 CPU i 4 GB pamięci RAM.
    Tutaj modelu uczyły się przez ponad 44 godziny.

    \noindent\begin{minipage}{\textwidth}
                 \begin{longtable}{|c|l|l|}
                     \caption{Parametry sieci}\\ \hline
                     \textbf{L.p.} & \textbf{Parametr} & \textbf{Wartości} \\ \hline
                     \endfirsthead
                     \multicolumn{3}{c}
                     {\tablename\ \thetable\ -- \textit{Parametry sieci - c.d.}} \\ \hline
                     \textbf{L.p.} & \textbf{Parametr} & \textbf{Wartości} \\ \hline
                     \endhead
                     \hline \multicolumn{3}{r}{\textit{Kontynuacja na następnej stronie}} \\
                     \endfoot
                     \hline
                     \endlastfoot
                     1 & architektura & \begin{tabular}[c]{@{}l@{}}
                                            1) 24, 16, 12, 8\\ 2) 16, 12, 8\\ 3) 16, 8\\ 4) 8
                     \end{tabular} \\ \hline
                     2 & funkcja aktywacji & \begin{tabular}[c]{@{}l@{}}
                                                 1) sigmoid\\ 2) tanh
                     \end{tabular} \\ \hline
                     3 & liczba cech & \begin{tabular}[c]{@{}l@{}}
                                           1) 30\\ 2) 20\\ 3) 10
                     \end{tabular} \\ \hline
                     4 & współczynnik uczenia & \begin{tabular}[c]{@{}l@{}}
                                                    1) od 0.2 do 0.2\\ 2) od 0.2 do 0.01\\ 3) od 0.2 do 0.001\\ 4) od 0.1 do 0.1\\ 5) od 0.1 do 0.01
                     \end{tabular} \\ \hline
                 \end{longtable}
    \end{minipage}

    Parametr „architektura” informuje ile jest warstw ukrytych i ile zawierają one neuronów.
    Każda sieć na końcu ma 8 neuronów, ponieważ jest 8 możliwych klas.

    Do badań użyłem różnej liczby cech.
    Są to 30, 20 lub 10 cech o najwyższej wartości chi2.

    „Współczynnik uczenia” to dwie liczby, wartość na początku uczenia i na końcu.
    Jeśli jest powtórzona liczba, znaczy to, że był stały.
    W przeciwnym wypadku zmieniał się liniowo z każdą kolejną epoką.

    \noindent\begin{minipage}{\textwidth}
                 \begin{longtable}{|c|l|l|l|l|l|}
                     \caption{Testowane modele}\\ \hline
                     \textbf{L.p.} & \textbf{Nazwa} & \textbf{Architektura}  & \textbf{\begin{tabular}[c]{@{}c@{}}Funkcja\\ aktywacji\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Liczba\\ cech\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Współczynnik\\ uczenia\end{tabular}} \\ \hline
                     \endfirsthead
                     \multicolumn{5}{c}
                     {\tablename\ \thetable\ -- \textit{Testowane modele - c.d.}} \\ \hline
                     \textbf{L.p.} & \textbf{Nazwa} & \textbf{Architektura}  & \textbf{\begin{tabular}[c]{@{}c@{}}Funkcja\\ aktywacji\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Liczba\\ cech\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Współczynnik\\ uczenia\end{tabular}} \\ \hline
                     \endhead
                     \hline \multicolumn{5}{r}{\textit{Kontynuacja na następnej stronie}} \\
                     \endfoot
                     \hline
                     \endlastfoot
                     1 & $M_{1}$ & 24\_16\_12\_8 & sigmoid & 30 & 0.2\_0.2 \\ \hline
                     2 & $M_{2}$ & 24\_16\_12\_8 & sigmoid & 30 & 0.2\_0.01 \\ \hline
                     3 & $M_{3}$ & 16\_8 & sigmoid & 10 & 0.2\_0.001 \\ \hline
                     4 & $M_{4}$ & 16\_8 & sigmoid & 20 & 0.2\_0.001 \\ \hline
                     5 & $M_{5}$ & 16\_8 & sigmoid & 30 & 0.2\_0.001 \\ \hline
                     6 & $M_{6}$ & 8 & sigmoid & 30 & 0.1\_0.01 \\ \hline
                     7 & $M_{7}$ & 16\_8 & sigmoid & 30 & 0.1\_0.01 \\ \hline
                     8 & $M_{8}$ & 16\_12\_8 & sigmoid & 30 & 0.1\_0.01 \\ \hline
                     9 & $M_{9}$ & 24\_16\_12\_8 & sigmoid & 30 & 0.1\_0.01 \\ \hline

                     10 & $M_{10}$ & 24\_16\_12\_8 & tanh & 10 & 0.1\_0.01 \\ \hline
                     11 & $M_{11}$ & 24\_16\_12\_8 & tanh & 20 & 0.1\_0.01 \\ \hline
                     12 & $M_{12}$ & 24\_16\_12\_8 & tanh & 30 & 0.1\_0.01 \\ \hline

                     13 & $M_{13}$ & 8 & tanh & 30 & 0.2\_0.2 \\ \hline
                     14 & $M_{14}$ & 16\_8 & tanh & 30 & 0.2\_0.2 \\ \hline
                     15 & $M_{15}$ & 16\_12\_8 & tanh & 30 & 0.2\_0.2 \\ \hline

                     16 & $M_{16}$ & 24\_16\_12\_8 & tanh & 30 & 0.1\_0.1 \\ \hline
                     17 & $M_{17}$ & 24\_16\_12\_8 & tanh & 30 & 0.2\_0.001 \\ \hline
                     18 & $M_{18}$ & 24\_16\_12\_8 & tanh & 30 & 0.2\_0.2 \\ \hline
                 \end{longtable}
    \end{minipage}

    \vspace{2em}

    \section{Badania z funkcją sigmoid}\label{sec:badaniaZFunkcjąSigmoid}

    \subsection{Różne współczynniki uczenia}\label{subsec:różneWspólczynnikiUczenia}

    Poniżej znajduje się tabela z przebiegu uczenia dwóch modeli o architekturze 24\_16\_12\_8, funkcji aktywacji sigmoid, uczonym na 30 cechach i różniącym się współczynnikiem uczenia.

    %    \noindent\begin{minipage}{\textwidth}
    \begin{longtable}{|c|l|l|l|l|}
        \caption{Porównianie modeli $M_1$ i $M_2$}\\ \hline
        & \multicolumn{2}{c|}{\textbf{$M_1$}} & \multicolumn{2}{c|}{\textbf{$M_2$}} \\ \hline
        \textbf{Epoka} & \textbf{Dokładność} & \textbf{Błąd}  & \textbf{Dokładność} & \textbf{Błąd} \\ \hline
        \endfirsthead
        \multicolumn{5}{c}
        {\tablename\ \thetable\ -- \textit{Porównianie modeli $M_1$ i $M_2$ - c.d.}} \\ \hline
        & \multicolumn{2}{c|}{\textbf{$M_1$}} & \multicolumn{2}{c|}{\textbf{$M_2$}} \\ \hline
        \textbf{Epoka} & \textbf{Dokładność} & \textbf{Błąd}  & \textbf{Dokładność} & \textbf{Błąd} \\ \hline
        \endhead
        \hline \multicolumn{5}{r}{\textit{Kontynuacja na następnej stronie}} \\
        \endfoot
        \hline
        \endlastfoot
        1 & 0.277 & 47.031 & 0.277 & 47.031 \\ \hline
        10 & 0.513 & 37.573 & 0.563 & 33.302 \\ \hline
        20 & 0.588 & 32.163 & 0.622 & 29.612 \\ \hline
        30 & 0.622 & 27.482 & 0.597 & 30.062 \\ \hline
        40 & 0.647 & 26.037 & 0.647 & 27.523 \\ \hline
        50 & 0.655 & 27.505 & 0.655 & 27.221 \\ \hline
        60 & 0.655 & 31.413 & 0.664 & 25.058 \\ \hline
        70 & 0.655 & 29.571 & 0.723 & 22.468 \\ \hline
        80 & 0.697 & 22.540 & 0.689 & 22.567 \\ \hline
        90 & 0.714 & 22.712 & 0.731 & 22.202 \\ \hline
        100 & 0.639 & 33.643 & 0.706 & 23.672 \\ \hline
        110 & 0.672 & 30.809 & 0.756 & 21.424 \\ \hline
        120 & 0.765 & 20.840 & 0.723 & 25.234 \\ \hline
        130 & 0.773 & 19.664 & 0.714 & 23.904 \\ \hline
        140 & 0.748 & 21.401 & 0.731 & 21.937 \\ \hline
        150 & 0.739 & 22.939 & 0.739 & 20.739 \\ \hline
        160 & 0.798 & 18.715 & 0.824 & 17.646 \\ \hline
        170 & 0.748 & 20.608 & 0.790 & 17.594 \\ \hline
        180 & 0.807 & 19.054 & 0.773 & 20.165 \\ \hline
        190 & 0.790 & 20.590 & 0.824 & 16.881 \\ \hline
        200 & 0.849 & 16.287 & 0.798 & 19.377 \\ \hline
        210 & 0.798 & 16.966 & 0.815 & 18.842 \\ \hline
        220 & 0.782 & 21.849 & 0.815 & 17.510 \\ \hline
        230 & 0.765 & 20.966 & 0.824 & 17.073 \\ \hline
        240 & 0.756 & 22.650 & 0.832 & 17.168 \\ \hline
        250 & 0.849 & 16.440 & 0.815 & 17.868 \\ \hline
        260 & 0.807 & 17.105 & 0.815 & 18.350 \\ \hline
        270 & 0.824 & 16.844 & 0.824 & 17.483 \\ \hline
        280 & 0.832 & 16.893 & 0.832 & 17.576 \\ \hline
        290 & 0.815 & 17.927 & 0.832 & 17.544 \\ \hline
        300 & 0.773 & 21.567 & 0.832 & 17.523 \\ \hline
        310 & 0.655 & 35.103 & 0.832 & 17.505 \\ \hline
        320 & 0.798 & 17.729 & 0.832 & 17.489 \\ \hline
        330 & 0.857 & 13.654 & 0.832 & 17.475 \\ \hline
        340 & 0.790 & 20.248 & 0.832 & 17.462 \\ \hline
        350 & 0.790 & 19.990 & 0.832 & 17.450 \\ \hline
        360 & 0.782 & 19.722 & 0.832 & 17.441 \\ \hline
        370 & 0.739 & 24.702 & 0.832 & 17.433 \\ \hline
        380 & 0.824 & 16.167 & 0.832 & 17.427 \\ \hline
        390 & 0.815 & 18.179 & 0.832 & 17.423 \\ \hline
        400 & 0.807 & 16.703 & 0.832 & 17.420 \\ \hline
    \end{longtable}
    %    \end{minipage}

    Wykres wartości błędu przedstawia błąd średniokwadratowy z procesu uczenia co 5 epokę.
    Trend funkcji jest mniej więcej podobny, ale widać, że dla przypadku ze stałym współczynnikiem uczenia wartość błędu spada mniej gwałtownie.
    Gdy zmniejsza się współczynnik uczenia trening przebiega bardzo agresywnie, czasem daje chwilowo lepsze wyniki, ale przez większość epok błąd jet większy.

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.8]{./img/lr-error.png}
        \caption{Porównanie wartości błędu modeli $M_1$ i $M_2$}
        %        \label{}
    \end{figure}

    Dla wykresu porówniania dokładności można wysnuć podobne wnioski, że lepiej jest zmniejszać współczynnik uczenia w trakcie nauki.
    Dzieje się tak dlatego, że po wielu epokach, gdy współczynnik uczenia dalej jest relatywnie duży gradient pomija minimum i zaczyna rosnąć.
    Oba modele osiągnęły wysoką dokładność ponad 80\%.

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.8]{./img/lr-accuracy.png}
        \caption{Porównanie dokładności modeli $M_1$ i $M_2$}
        %        \label{}
    \end{figure}

    \subsection{Różna liczba cech}\label{subsec:różnaLiczbaCech}

    Drugie porównanie to modele o architekturze 16\_8, funkcji aktywacji sigmoid.
    Trenowane były z malejącym współczynnikiem uczenia.
    Parametr, który je różni, to liczba cech.
    $M_3$ klasyfikuje korzystając z 10 cech o najwyższym współczynniku chi2.
    $M_4$ dostaje 20 cech, a $M_5$ uczy się na prawie wszystkich cechach, czyli 30 najlepszych.

    %    \noindent\begin{minipage}{\textwidth}
    \begin{longtable}{|c|l|l|l|l|l|l|}
        \caption{Porównianie modeli $M_3$, $M_4$ i $M_5$}\\ \hline
        & \multicolumn{2}{c|}{\textbf{$M_3$}} & \multicolumn{2}{c|}{\textbf{$M_4$}} & \multicolumn{2}{c|}{\textbf{$M_5$}} \\ \hline
        \textbf{Epoka} & \textbf{Dokładność} & \textbf{Błąd} & \textbf{Dokładność} & \textbf{Błąd} & \textbf{Dokładność} & \textbf{Błąd} \\ \hline
        \endfirsthead
        \multicolumn{7}{c}
        {\tablename\ \thetable\ -- \textit{Porównianie modeli $M_3$, $M_4$ i $M_5$ - c.d.}} \\ \hline
        & \multicolumn{2}{c|}{\textbf{$M_3$}} & \multicolumn{2}{c|}{\textbf{$M_4$}} & \multicolumn{2}{c|}{\textbf{$M_5$}} \\ \hline
        \textbf{Epoka} & \textbf{Dokładność} & \textbf{Błąd} & \textbf{Dokładność} & \textbf{Błąd} & \textbf{Dokładność} & \textbf{Błąd} \\ \hline
        \endhead
        \hline \multicolumn{7}{r}{\textit{Kontynuacja na następnej stronie}} \\
        \endfoot
        \hline
        \endlastfoot
        1 & 0.538 & 41.299 & 0.496 & 41.714 & 0.462 & 43.907 \\ \hline
        10 & 0.571 & 34.109 & 0.613 & 26.876 & 0.613 & 28.462 \\ \hline
        20 & 0.647 & 29.445 & 0.714 & 24.232 & 0.655 & 23.769 \\ \hline
        30 & 0.664 & 27.188 & 0.723 & 20.451 & 0.765 & 20.308 \\ \hline
        40 & 0.672 & 26.045 & 0.782 & 17.584 & 0.765 & 18.426 \\ \hline
        50 & 0.681 & 23.634 & 0.782 & 16.687 & 0.824 & 15.077 \\ \hline
        60 & 0.739 & 21.233 & 0.790 & 16.759 & 0.849 & 14.902 \\ \hline
        70 & 0.773 & 20.575 & 0.832 & 15.302 & 0.849 & 13.756 \\ \hline
        80 & 0.765 & 22.901 & 0.849 & 14.367 & 0.874 & 12.665 \\ \hline
        90 & 0.739 & 21.739 & 0.849 & 13.351 & 0.866 & 12.680 \\ \hline
        100 & 0.765 & 21.240 & 0.857 & 13.708 & 0.857 & 12.414 \\ \hline
        110 & 0.739 & 20.959 & 0.832 & 14.686 & 0.874 & 11.797 \\ \hline
        120 & 0.756 & 20.595 & 0.866 & 13.803 & 0.874 & 11.426 \\ \hline
        130 & 0.773 & 20.562 & 0.874 & 12.516 & 0.882 & 11.068 \\ \hline
        140 & 0.798 & 20.210 & 0.874 & 12.519 & 0.891 & 10.854 \\ \hline
        150 & 0.798 & 19.903 & 0.866 & 13.173 & 0.899 & 10.735 \\ \hline
        160 & 0.798 & 19.924 & 0.882 & 12.943 & 0.908 & 10.647 \\ \hline
        170 & 0.807 & 19.784 & 0.866 & 13.364 & 0.908 & 10.576 \\ \hline
        180 & 0.807 & 19.581 & 0.849 & 13.898 & 0.908 & 10.529 \\ \hline
        190 & 0.815 & 19.234 & 0.849 & 14.285 & 0.908 & 10.507 \\ \hline
        200 & 0.807 & 18.782 & 0.849 & 13.564 & 0.908 & 10.536 \\ \hline
        210 & 0.798 & 18.772 & 0.849 & 13.768 & 0.908 & 10.761 \\ \hline
        220 & 0.782 & 18.848 & 0.857 & 13.938 & 0.874 & 11.259 \\ \hline
        230 & 0.765 & 19.123 & 0.857 & 13.972 & 0.882 & 10.923 \\ \hline
        240 & 0.773 & 18.967 & 0.866 & 13.971 & 0.908 & 10.914 \\ \hline
        250 & 0.790 & 18.496 & 0.866 & 13.969 & 0.908 & 10.813 \\ \hline
        260 & 0.790 & 18.480 & 0.866 & 13.976 & 0.899 & 10.745 \\ \hline
        270 & 0.798 & 18.272 & 0.866 & 13.991 & 0.899 & 10.722 \\ \hline
        280 & 0.807 & 18.269 & 0.857 & 14.007 & 0.891 & 10.729 \\ \hline
        290 & 0.798 & 18.229 & 0.857 & 14.019 & 0.891 & 10.752 \\ \hline
        300 & 0.798 & 18.170 & 0.857 & 14.027 & 0.891 & 10.780 \\ \hline
        310 & 0.807 & 18.101 & 0.857 & 14.034 & 0.891 & 10.808 \\ \hline
        320 & 0.807 & 18.028 & 0.857 & 14.040 & 0.891 & 10.834 \\ \hline
        330 & 0.807 & 17.961 & 0.857 & 14.043 & 0.891 & 10.858 \\ \hline
        340 & 0.815 & 17.909 & 0.857 & 14.043 & 0.891 & 10.880 \\ \hline
        350 & 0.815 & 17.824 & 0.857 & 14.042 & 0.891 & 10.899 \\ \hline
        360 & 0.815 & 17.725 & 0.857 & 14.040 & 0.891 & 10.917 \\ \hline
        370 & 0.815 & 17.647 & 0.857 & 14.038 & 0.891 & 10.933 \\ \hline
        380 & 0.807 & 17.589 & 0.857 & 14.035 & 0.891 & 10.948 \\ \hline
        390 & 0.807 & 17.545 & 0.857 & 14.032 & 0.891 & 10.962 \\ \hline
        400 & 0.807 & 17.523 & 0.857 & 14.032 & 0.891 & 10.972 \\ \hline
    \end{longtable}
    %    \end{minipage}

    Z wykresu porównania wartości błędu uczenia tych modeli można wywnioskować, że liczba cech ma wpływ na jakość klasyfikacji.
    Im więcej cech, tym model popełnia mniejszy błąd.
    Od dwusetnej epoki błąd maleje bardzo wolno, dlatego można by przerwać wcześniej uczenie bez straty na jakości klasyfikacji.

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.8]{./img/feat-error.png}
        \caption{Porównanie wartości błędu modeli $M_3$, $M_4$ i $M_5$}
        %        \label{}
    \end{figure}

    Wykres dokładności od epoki potwierdza, że dla tego zbioru danych warto użyć większej liczby cech, kosztem większego i bardziej skomplikowanego modelu.
    Tak jak na poprzednim wykresie, tutaj też po dwusetnej nie widać żadnej poprawy, a nawet obniżenie jakości predykcji może wskazywać na przeuczenie klasyfikatora.
    To znaczy, że za bardzo dostosował się do danych uczących tracąc zdolność do uogólniania.

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.8]{./img/feat-accuracy.png}
        \caption{Porównanie dokładności modeli $M_3$, $M_4$ i $M_5$}
        %        \label{}
    \end{figure}

    \subsection{Różne architektury}\label{subsec:różneArchitektury}

    Kolejnej analizie poddano sieci neuronowe mające funkcję aktywacji sigmoid oraz 30 neuronów w warstwie wejściowej.
    Współczynnik uczenia malał od $1.0$ do $0.01$ we wszystkich modelach tak samo.
    Kolejne klasyfikatory miały następujące architektury 8, 16\_8, 16\_12\_8 i 24\_16\_12\_8.

    Wydawać by się mogło, że im więcej warstw ukrytych, tym model dokonuje klasyfikacji z wyższą dokładnością.

    %    \noindent\begin{minipage}{\textwidth}
    \begin{longtable}{|c|l|l|l|l|l|l|l|l|}
        \caption{Porównianie modeli $M_6$, $M_7$, $M_8$ i $M_9$}\\ \hline
        & \multicolumn{2}{c|}{\textbf{$M_6$}} & \multicolumn{2}{c|}{\textbf{$M_7$}} & \multicolumn{2}{c|}{\textbf{$M_8$}} & \multicolumn{2}{c|}{\textbf{$M_9$}} \\ \hline
        \textbf{Epoka} & \textbf{Dokł.} & \textbf{Błąd} & \textbf{Dokł.} & \textbf{Błąd} & \textbf{Dokł.} & \textbf{Błąd} & \textbf{Dokł.} & \textbf{Błąd} \\ \hline
        \endfirsthead
        \multicolumn{9}{c}
        {\tablename\ \thetable\ -- \textit{Porównianie modeli $M_6$, $M_7$, $M_8$ i $M_9$ - c.d.}} \\ \hline
        & \multicolumn{2}{c|}{\textbf{$M_6$}} & \multicolumn{2}{c|}{\textbf{$M_7$}} & \multicolumn{2}{c|}{\textbf{$M_8$}} & \multicolumn{2}{c|}{\textbf{$M_9$}} \\ \hline
        \textbf{Epoka} & \textbf{Dokł.} & \textbf{Błąd} & \textbf{Dokł.} & \textbf{Błąd} & \textbf{Dokł.} & \textbf{Błąd} & \textbf{Dokł.} & \textbf{Błąd} \\ \hline
        \endhead
        \hline \multicolumn{9}{r}{\textit{Kontynuacja na następnej stronie}} \\
        \endfoot
        \hline
        \endlastfoot
        1 & 0.361 & 54.950 & 0.420 & 45.093 & 0.387 & 46.705 & 0.387 & 46.968 \\ \hline
        10 & 0.630 & 28.059 & 0.605 & 28.339 & 0.563 & 33.505 & 0.521 & 46.049 \\ \hline
        20 & 0.647 & 28.050 & 0.655 & 26.151 & 0.630 & 29.422 & 0.580 & 31.339 \\ \hline
        30 & 0.697 & 26.420 & 0.723 & 22.149 & 0.647 & 27.208 & 0.630 & 29.242 \\ \hline
        40 & 0.681 & 27.203 & 0.756 & 19.402 & 0.672 & 25.992 & 0.613 & 28.203 \\ \hline
        50 & 0.697 & 26.257 & 0.824 & 16.846 & 0.697 & 24.958 & 0.630 & 27.234 \\ \hline
        60 & 0.697 & 26.023 & 0.832 & 15.827 & 0.681 & 23.236 & 0.647 & 27.873 \\ \hline
        70 & 0.706 & 25.647 & 0.832 & 15.026 & 0.681 & 22.635 & 0.639 & 27.101 \\ \hline
        80 & 0.714 & 25.314 & 0.840 & 14.182 & 0.714 & 22.061 & 0.647 & 26.490 \\ \hline
        90 & 0.723 & 24.919 & 0.840 & 13.757 & 0.748 & 21.656 & 0.655 & 27.194 \\ \hline
        100 & 0.731 & 24.451 & 0.840 & 13.870 & 0.756 & 20.421 & 0.664 & 26.650 \\ \hline
        110 & 0.723 & 24.131 & 0.849 & 13.930 & 0.773 & 19.475 & 0.664 & 26.430 \\ \hline
        120 & 0.731 & 23.937 & 0.840 & 13.729 & 0.748 & 18.962 & 0.664 & 26.588 \\ \hline
        130 & 0.723 & 24.139 & 0.840 & 13.372 & 0.765 & 18.062 & 0.681 & 25.698 \\ \hline
        140 & 0.714 & 24.308 & 0.840 & 13.044 & 0.756 & 18.048 & 0.672 & 25.671 \\ \hline
        150 & 0.714 & 24.331 & 0.840 & 12.807 & 0.756 & 18.073 & 0.655 & 25.225 \\ \hline
        160 & 0.723 & 24.246 & 0.849 & 12.668 & 0.782 & 17.879 & 0.681 & 24.412 \\ \hline
        170 & 0.731 & 24.122 & 0.849 & 12.622 & 0.782 & 17.810 & 0.723 & 23.230 \\ \hline
        180 & 0.739 & 23.974 & 0.849 & 12.606 & 0.790 & 17.619 & 0.739 & 22.015 \\ \hline
        190 & 0.739 & 23.798 & 0.849 & 12.606 & 0.807 & 17.330 & 0.748 & 21.055 \\ \hline
        200 & 0.739 & 23.590 & 0.857 & 12.630 & 0.815 & 17.058 & 0.765 & 20.394 \\ \hline
        210 & 0.748 & 23.349 & 0.857 & 12.693 & 0.815 & 16.827 & 0.782 & 19.212 \\ \hline
        220 & 0.748 & 23.076 & 0.857 & 12.782 & 0.815 & 16.665 & 0.790 & 18.236 \\ \hline
        230 & 0.748 & 22.790 & 0.857 & 13.083 & 0.807 & 16.587 & 0.798 & 17.815 \\ \hline
        240 & 0.748 & 22.544 & 0.849 & 13.245 & 0.807 & 16.578 & 0.798 & 17.523 \\ \hline
        250 & 0.765 & 22.413 & 0.849 & 13.167 & 0.815 & 16.604 & 0.798 & 17.386 \\ \hline
        260 & 0.748 & 22.411 & 0.857 & 13.130 & 0.815 & 16.627 & 0.798 & 17.369 \\ \hline
        270 & 0.731 & 22.462 & 0.849 & 13.109 & 0.824 & 16.603 & 0.798 & 17.412 \\ \hline
        280 & 0.731 & 22.496 & 0.840 & 13.083 & 0.815 & 16.538 & 0.798 & 17.466 \\ \hline
        290 & 0.739 & 22.517 & 0.840 & 13.056 & 0.815 & 16.464 & 0.807 & 17.510 \\ \hline
        300 & 0.748 & 22.535 & 0.840 & 13.031 & 0.815 & 16.395 & 0.807 & 17.540 \\ \hline
        310 & 0.748 & 22.553 & 0.840 & 13.008 & 0.815 & 16.340 & 0.790 & 17.565 \\ \hline
        320 & 0.739 & 22.572 & 0.849 & 12.986 & 0.815 & 16.305 & 0.790 & 17.598 \\ \hline
        330 & 0.739 & 22.591 & 0.849 & 12.966 & 0.815 & 16.291 & 0.790 & 17.628 \\ \hline
        340 & 0.731 & 22.608 & 0.849 & 12.947 & 0.815 & 16.294 & 0.790 & 17.612 \\ \hline
        350 & 0.731 & 22.622 & 0.857 & 12.930 & 0.815 & 16.310 & 0.790 & 17.569 \\ \hline
        360 & 0.731 & 22.633 & 0.857 & 12.916 & 0.815 & 16.335 & 0.790 & 17.530 \\ \hline
        370 & 0.731 & 22.639 & 0.857 & 12.906 & 0.815 & 16.367 & 0.790 & 17.497 \\ \hline
        380 & 0.731 & 22.641 & 0.857 & 12.900 & 0.815 & 16.407 & 0.790 & 17.469 \\ \hline
        390 & 0.731 & 22.640 & 0.857 & 12.897 & 0.815 & 16.454 & 0.790 & 17.447 \\ \hline
        400 & 0.731 & 22.636 & 0.866 & 12.898 & 0.815 & 16.509 & 0.790 & 17.429 \\ \hline
    \end{longtable}
    %    \end{minipage}

    Wykres wartości błędu pokazuje, że bardziej skomplikowana architekrura wcale nie zapewnia lepszych wyników.
    Największy model $M_9$ przez prawie połowę okresu uczenia radził sobie najgorzej, a na koniec wyprzedził najmniejszy model $M_6$.

    Najlepsze wyniki osiągnięto z jedną warstwą ukrytą w modelu $M_7$, potem z dwoma warstwami ukrytymi $M_8$.

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.8]{./img/arch-error.png}
        \caption{Porównanie wartości błędu modeli $M_6$, $M_7$, $M_8$ i $M_9$}
        %        \label{}
    \end{figure}

    Zależność dokładności od epoki nie zmienia sytuacji.
    Model bez warstw ukrytych osiągnął ponad 73\% dokładności, co było zaskoczeniem.

    Ponownie można było przerwać uczenie w okolicy epoki numer 200, gdyż dalsze trenowanie nie poprawiało rezultatów.

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.8]{./img/arch-accuracy.png}
        \caption{Porównanie dokładności modeli $M_6$, $M_7$, $M_8$ i $M_9$}
        %        \label{}
    \end{figure}

    \section{Badania z funkcją tanh}\label{sec:badaniaZFunkcjąTanh}

    \subsection{Różna liczba cech}\label{subsec:różnaLiczbaCech2}

    W drugiej części przeanalizowano modele z inną funkcją aktywacji - tanh.

    Porównano sieci o architekturze 24\_16\_12\_8 z malejącym współczynnikiem uczenia od $0.1$ do $0.01$.
    Klasyfikatory różniły się ponownie liczbą cech.

    %    \noindent\begin{minipage}{\textwidth}
    \begin{longtable}{|c|l|l|l|l|l|l|}
        \caption{Porównianie modeli $M_{10}$, $M_{11}$ i $M_{12}$}\\ \hline
        & \multicolumn{2}{c|}{\textbf{$M_{10}$}} & \multicolumn{2}{c|}{\textbf{$M_{11}$}} & \multicolumn{2}{c|}{\textbf{$M_{12}$}} \\ \hline
        \textbf{Epoka} & \textbf{Dokładność} & \textbf{Błąd} & \textbf{Dokładność} & \textbf{Błąd} & \textbf{Dokładność} & \textbf{Błąd} \\ \hline
        \endfirsthead
        \multicolumn{7}{c}
        {\tablename\ \thetable\ -- \textit{Porównianie modeli $M_{10}$, $M_{11}$ i $M_{12}$ - c.d.}} \\ \hline
        & \multicolumn{2}{c|}{\textbf{$M_{10}$}} & \multicolumn{2}{c|}{\textbf{$M_{11}$}} & \multicolumn{2}{c|}{\textbf{$M_{12}$}} \\ \hline
        \textbf{Epoka} & \textbf{Dokładność} & \textbf{Błąd} & \textbf{Dokładność} & \textbf{Błąd} & \textbf{Dokładność} & \textbf{Błąd} \\ \hline
        \endhead
        \hline \multicolumn{7}{r}{\textit{Kontynuacja na następnej stronie}} \\
        \endfoot
        \hline
        \endlastfoot
        1 & 0.353 & 52.640 & 0.277 & 55.873 & 0.118 & 55.034 \\ \hline
        10 & 0.538 & 39.613 & 0.487 & 39.530 & 0.345 & 52.538 \\ \hline
        20 & 0.571 & 33.717 & 0.580 & 33.959 & 0.555 & 36.910 \\ \hline
        30 & 0.605 & 33.379 & 0.597 & 32.794 & 0.664 & 32.991 \\ \hline
        40 & 0.664 & 33.391 & 0.672 & 28.346 & 0.580 & 35.834 \\ \hline
        50 & 0.613 & 35.795 & 0.630 & 30.813 & 0.630 & 33.222 \\ \hline
        60 & 0.689 & 30.368 & 0.655 & 27.470 & 0.655 & 30.992 \\ \hline
        70 & 0.647 & 30.664 & 0.756 & 24.646 & 0.605 & 34.815 \\ \hline
        80 & 0.723 & 25.034 & 0.664 & 27.952 & 0.622 & 35.889 \\ \hline
        90 & 0.613 & 34.623 & 0.756 & 20.232 & 0.622 & 32.761 \\ \hline
        100 & 0.689 & 25.934 & 0.748 & 24.249 & 0.672 & 27.784 \\ \hline
        110 & 0.790 & 20.253 & 0.748 & 20.231 & 0.639 & 35.127 \\ \hline
        120 & 0.773 & 24.424 & 0.756 & 22.806 & 0.647 & 31.005 \\ \hline
        130 & 0.706 & 26.605 & 0.765 & 20.004 & 0.538 & 35.759 \\ \hline
        140 & 0.739 & 24.553 & 0.697 & 25.398 & 0.664 & 31.898 \\ \hline
        150 & 0.790 & 20.773 & 0.697 & 27.043 & 0.714 & 25.219 \\ \hline
        160 & 0.639 & 29.552 & 0.714 & 24.707 & 0.714 & 25.421 \\ \hline
        170 & 0.790 & 20.973 & 0.731 & 22.933 & 0.706 & 27.352 \\ \hline
        180 & 0.807 & 20.067 & 0.723 & 24.532 & 0.714 & 25.905 \\ \hline
        190 & 0.782 & 21.582 & 0.765 & 23.374 & 0.647 & 37.363 \\ \hline
        200 & 0.756 & 24.949 & 0.773 & 20.865 & 0.739 & 23.828 \\ \hline
        210 & 0.773 & 20.389 & 0.790 & 17.957 & 0.756 & 24.335 \\ \hline
        220 & 0.798 & 22.621 & 0.739 & 23.427 & 0.739 & 23.932 \\ \hline
        230 & 0.773 & 24.285 & 0.773 & 21.307 & 0.765 & 23.108 \\ \hline
        240 & 0.790 & 22.683 & 0.798 & 20.925 & 0.731 & 25.638 \\ \hline
        250 & 0.765 & 22.440 & 0.773 & 21.445 & 0.756 & 22.974 \\ \hline
        260 & 0.798 & 23.804 & 0.790 & 17.580 & 0.723 & 24.399 \\ \hline
        270 & 0.782 & 21.929 & 0.832 & 16.134 & 0.790 & 22.530 \\ \hline
        280 & 0.798 & 21.549 & 0.832 & 16.583 & 0.790 & 21.926 \\ \hline
        290 & 0.798 & 20.037 & 0.815 & 17.821 & 0.773 & 21.182 \\ \hline
        300 & 0.807 & 20.206 & 0.815 & 17.117 & 0.782 & 22.278 \\ \hline
        310 & 0.798 & 20.617 & 0.815 & 16.688 & 0.765 & 22.705 \\ \hline
        320 & 0.807 & 20.875 & 0.824 & 17.641 & 0.765 & 25.037 \\ \hline
        330 & 0.815 & 20.677 & 0.824 & 17.042 & 0.815 & 19.822 \\ \hline
        340 & 0.807 & 20.595 & 0.824 & 17.087 & 0.798 & 22.790 \\ \hline
        350 & 0.807 & 20.455 & 0.824 & 17.412 & 0.824 & 20.356 \\ \hline
        360 & 0.807 & 20.403 & 0.824 & 17.062 & 0.807 & 21.937 \\ \hline
        370 & 0.807 & 20.450 & 0.832 & 17.048 & 0.824 & 18.904 \\ \hline
        380 & 0.807 & 20.319 & 0.832 & 17.427 & 0.815 & 21.773 \\ \hline
        390 & 0.807 & 19.992 & 0.832 & 17.430 & 0.815 & 21.767 \\ \hline
        400 & 0.815 & 19.859 & 0.824 & 16.917 & 0.815 & 21.706 \\ \hline
    \end{longtable}
    %    \end{minipage}

    Wykres wartości błędu pokazuje niespodziewanie, że ucząc model na 20 cechach otrzymano najmniejszy błąd.
    10 cech daje również lepszy wynik niż 30, choć tu różnica jest mniejsza.

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.8]{./img/feat-error-tanh.png}
        \caption{Porównanie wartości błędu modeli $M_{10}$, $M_{11}$ i $M_{12}$}
        %        \label{}
    \end{figure}

    Wykres dokładności nie potwierdza wyraźnie tego wniosku, ale i tutaj model $M_{11}$ radzi sobie nieznacznie lepiej niż inne.

    Wszystkie 3 modele przekroczyły nieznacznie próg 80\%.

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.8]{./img/feat-accuracy-tanh.png}
        \caption{Porównanie dokładności modeli $M_{10}$, $M_{11}$ i $M_{12}$}
        %        \label{}
    \end{figure}

    \subsection{Różne architektury}\label{subsec:różneArchitektury2}

    Zbadano proces ucznia sieci o funkcji aktywacji tanh i stałym współczynniku uczenia dla 30 cech.
    Porównano modele o architekturach bez warstwy ukrytej, z jedną i z dwoma warstwami ukrytymi.

    %    \noindent\begin{minipage}{\textwidth}
    \begin{longtable}{|c|l|l|l|l|l|l|}
        \caption{Porównianie modeli $M_{13}$, $M_{14}$ i $M_{15}$}\\ \hline
        & \multicolumn{2}{c|}{\textbf{$M_{13}$}} & \multicolumn{2}{c|}{\textbf{$M_{14}$}} & \multicolumn{2}{c|}{\textbf{$M_{15}$}} \\ \hline
        \textbf{Epoka} & \textbf{Dokładność} & \textbf{Błąd} & \textbf{Dokładność} & \textbf{Błąd} & \textbf{Dokładność} & \textbf{Błąd} \\ \hline
        \endfirsthead
        \multicolumn{7}{c}
        {\tablename\ \thetable\ -- \textit{Porównianie modeli $M_{13}$, $M_{14}$ i $M_{15}$ - c.d.}} \\ \hline
        & \multicolumn{2}{c|}{\textbf{$M_{13}$}} & \multicolumn{2}{c|}{\textbf{$M_{14}$}} & \multicolumn{2}{c|}{\textbf{$M_{15}$}} \\ \hline
        \textbf{Epoka} & \textbf{Dokładność} & \textbf{Błąd} & \textbf{Dokładność} & \textbf{Błąd} & \textbf{Dokładność} & \textbf{Błąd} \\ \hline
        \endhead
        \hline \multicolumn{7}{r}{\textit{Kontynuacja na następnej stronie}} \\
        \endfoot
        \hline
        \endlastfoot
        1 & 0.202 & 548.499 & 0.050 & 287.852 & 0.118 & 63.359 \\ \hline
        10 & 0.202 & 576.500 & 0.328 & 237.090 & 0.319 & 48.922 \\ \hline
        20 & 0.193 & 576.500 & 0.118 & 277.368 & 0.294 & 48.806 \\ \hline
        30 & 0.277 & 576.500 & 0.118 & 220.278 & 0.328 & 48.924 \\ \hline
        40 & 0.277 & 576.500 & 0.118 & 271.187 & 0.328 & 49.436 \\ \hline
        50 & 0.277 & 576.500 & 0.277 & 268.174 & 0.538 & 47.290 \\ \hline
        60 & 0.277 & 576.500 & 0.328 & 259.154 & 0.353 & 50.235 \\ \hline
        70 & 0.277 & 576.500 & 0.328 & 292.961 & 0.328 & 48.984 \\ \hline
        80 & 0.277 & 576.500 & 0.277 & 259.343 & 0.395 & 48.063 \\ \hline
        90 & 0.277 & 576.500 & 0.277 & 247.395 & 0.328 & 48.988 \\ \hline
        100 & 0.277 & 576.499 & 0.277 & 278.024 & 0.328 & 48.974 \\ \hline
        110 & 0.277 & 560.500 & 0.118 & 293.126 & 0.328 & 48.972 \\ \hline
        120 & 0.277 & 560.500 & 0.118 & 318.174 & 0.328 & 47.771 \\ \hline
        130 & 0.277 & 560.500 & 0.118 & 317.897 & 0.328 & 48.945 \\ \hline
        140 & 0.277 & 560.500 & 0.328 & 246.111 & 0.328 & 48.834 \\ \hline
        150 & 0.277 & 560.500 & 0.328 & 293.770 & 0.328 & 48.849 \\ \hline
        160 & 0.277 & 560.500 & 0.328 & 276.355 & 0.521 & 45.437 \\ \hline
        170 & 0.277 & 560.500 & 0.067 & 308.603 & 0.303 & 51.606 \\ \hline
        180 & 0.277 & 560.500 & 0.328 & 282.134 & 0.328 & 48.819 \\ \hline
        190 & 0.277 & 560.500 & 0.118 & 269.000 & 0.353 & 48.910 \\ \hline
        200 & 0.277 & 560.500 & 0.277 & 252.517 & 0.328 & 48.791 \\ \hline
        210 & 0.277 & 560.500 & 0.067 & 311.607 & 0.328 & 48.796 \\ \hline
        220 & 0.277 & 560.499 & 0.084 & 284.886 & 0.328 & 48.796 \\ \hline
        230 & 0.277 & 560.499 & 0.277 & 275.749 & 0.328 & 48.795 \\ \hline
        240 & 0.277 & 560.499 & 0.118 & 308.011 & 0.328 & 48.795 \\ \hline
        250 & 0.277 & 560.498 & 0.328 & 265.692 & 0.328 & 48.796 \\ \hline
        260 & 0.277 & 560.466 & 0.328 & 257.700 & 0.328 & 48.798 \\ \hline
        270 & 0.277 & 560.500 & 0.328 & 282.665 & 0.328 & 48.801 \\ \hline
        280 & 0.277 & 560.500 & 0.118 & 259.123 & 0.328 & 48.801 \\ \hline
        290 & 0.277 & 540.500 & 0.118 & 235.230 & 0.328 & 48.802 \\ \hline
        300 & 0.277 & 540.500 & 0.118 & 266.620 & 0.328 & 48.800 \\ \hline
        310 & 0.277 & 540.500 & 0.050 & 276.539 & 0.328 & 48.798 \\ \hline
        320 & 0.277 & 540.500 & 0.328 & 271.965 & 0.328 & 48.796 \\ \hline
        330 & 0.277 & 540.500 & 0.328 & 253.050 & 0.328 & 48.794 \\ \hline
        340 & 0.277 & 540.500 & 0.118 & 221.669 & 0.328 & 48.794 \\ \hline
        350 & 0.277 & 540.499 & 0.118 & 216.252 & 0.328 & 48.797 \\ \hline
        360 & 0.277 & 540.499 & 0.328 & 223.183 & 0.328 & 48.799 \\ \hline
        370 & 0.277 & 540.499 & 0.118 & 250.361 & 0.328 & 48.802 \\ \hline
        380 & 0.277 & 540.499 & 0.118 & 259.041 & 0.328 & 48.803 \\ \hline
        390 & 0.277 & 540.499 & 0.277 & 256.229 & 0.328 & 48.802 \\ \hline
        400 & 0.277 & 540.499 & 0.118 & 260.246 & 0.328 & 48.800 \\ \hline
    \end{longtable}
    %    \end{minipage}

    Przebieg wartości błędu pokazuje, że te modele w ogóle się nie uczyły.
    Błąd jest stały dla modeli $M_{13}$ i $M_{15}$.
    Model $M_{14}$ oscyluje wokół wartości 300, ale również się nie poprawia.

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.8]{./img/arch-error-tanh.png}
        \caption{Porównanie wartości błędu modeli $M_{13}$, $M_{14}$ i $M_{15}$}
        %        \label{}
    \end{figure}

    Wykres dokładności potwierdza brak możliwości nauki tych modeli.
    Jak na poprzednim wykresie model bez warstwy ukrytej i ten z dwiema warstwami mają praktycznie stałe wartości dokładności.
    Model $M_{15}$ jest wciąż trochę lepszy niż $M_{13}$ o około 5 punktów procentowych.

    Model z jedną warstwą ukrytą intensywnie zmienia wagi, dlatego dokładność skacze w zakresie 5--32\%, co również jest bardzo małą skutecznością.

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.8]{./img/arch-accuracy-tanh.png}
        \caption{Porównanie dokładności modeli $M_{13}$, $M_{14}$ i $M_{15}$}
        %        \label{}
    \end{figure}

    \subsection{Różne współczynniki uczenia}\label{subsec:różneWspólczynnikiUczenia2}

    Ostatnie badanie to największe modele z funkcją aktywacji tanh, uczone z na zbiorze danych zawierający 30 cech.
    Kryterium porównania to współczynnik uczenia.

    Poniżej znajduje się tabela z wynikami przebiegu uczenia.

    %    \noindent\begin{minipage}{\textwidth}
    \begin{longtable}{|c|l|l|l|l|l|l|}
        \caption{Porównianie modeli $M_{16}$, $M_{17}$ i $M_{18}$}\\ \hline
        & \multicolumn{2}{c|}{\textbf{$M_{16}$}} & \multicolumn{2}{c|}{\textbf{$M_{17}$}} & \multicolumn{2}{c|}{\textbf{$M_{18}$}} \\ \hline
        \textbf{Epoka} & \textbf{Dokładność} & \textbf{Błąd} & \textbf{Dokładność} & \textbf{Błąd} & \textbf{Dokładność} & \textbf{Błąd} \\ \hline
        \endfirsthead
        \multicolumn{7}{c}
        {\tablename\ \thetable\ -- \textit{Porównianie modeli $M_{16}$, $M_{17}$ i $M_{18}$ - c.d.}} \\ \hline
        & \multicolumn{2}{c|}{\textbf{$M_{16}$}} & \multicolumn{2}{c|}{\textbf{$M_{17}$}} & \multicolumn{2}{c|}{\textbf{$M_{18}$}} \\ \hline
        \textbf{Epoka} & \textbf{Dokładność} & \textbf{Błąd} & \textbf{Dokładność} & \textbf{Błąd} & \textbf{Dokładność} & \textbf{Błąd} \\ \hline
        \endhead
        \hline \multicolumn{7}{r}{\textit{Kontynuacja na następnej stronie}} \\
        \endfoot
        \hline
        \endlastfoot
        1 & 0.118 & 55.034 & 0.328 & 48.990 & 0.328 & 48.990 \\ \hline
        10 & 0.361 & 57.834 & 0.328 & 48.993 & 0.328 & 49.043 \\ \hline
        20 & 0.471 & 46.837 & 0.328 & 49.025 & 0.328 & 48.993 \\ \hline
        30 & 0.605 & 33.761 & 0.328 & 48.648 & 0.328 & 48.781 \\ \hline
        40 & 0.555 & 36.765 & 0.370 & 48.121 & 0.328 & 48.797 \\ \hline
        50 & 0.378 & 37.930 & 0.328 & 48.510 & 0.328 & 49.164 \\ \hline
        60 & 0.622 & 32.719 & 0.328 & 48.457 & 0.328 & 48.763 \\ \hline
        70 & 0.529 & 48.949 & 0.328 & 48.408 & 0.328 & 48.762 \\ \hline
        80 & 0.613 & 32.237 & 0.328 & 48.331 & 0.328 & 48.783 \\ \hline
        90 & 0.622 & 33.052 & 0.437 & 47.559 & 0.328 & 48.777 \\ \hline
        100 & 0.605 & 31.709 & 0.496 & 44.356 & 0.328 & 48.777 \\ \hline
        110 & 0.630 & 34.209 & 0.328 & 48.212 & 0.328 & 48.777 \\ \hline
        120 & 0.613 & 34.724 & 0.328 & 48.191 & 0.328 & 48.777 \\ \hline
        130 & 0.597 & 31.240 & 0.328 & 48.152 & 0.328 & 48.777 \\ \hline
        140 & 0.588 & 37.751 & 0.328 & 48.114 & 0.328 & 48.777 \\ \hline
        150 & 0.639 & 30.896 & 0.328 & 48.078 & 0.336 & 49.257 \\ \hline
        160 & 0.622 & 32.941 & 0.328 & 48.042 & 0.328 & 48.623 \\ \hline
        170 & 0.571 & 35.961 & 0.328 & 48.006 & 0.361 & 48.611 \\ \hline
        180 & 0.630 & 34.751 & 0.277 & 47.972 & 0.328 & 48.776 \\ \hline
        190 & 0.605 & 33.902 & 0.277 & 47.937 & 0.328 & 48.775 \\ \hline
        200 & 0.605 & 43.547 & 0.277 & 47.902 & 0.328 & 48.775 \\ \hline
        210 & 0.647 & 31.029 & 0.277 & 47.867 & 0.361 & 48.744 \\ \hline
        220 & 0.613 & 36.711 & 0.277 & 47.830 & 0.328 & 48.777 \\ \hline
        230 & 0.630 & 32.052 & 0.277 & 47.793 & 0.328 & 48.777 \\ \hline
        240 & 0.622 & 32.695 & 0.277 & 47.754 & 0.328 & 48.776 \\ \hline
        250 & 0.630 & 32.104 & 0.277 & 47.713 & 0.328 & 48.776 \\ \hline
        260 & 0.445 & 57.777 & 0.277 & 47.670 & 0.328 & 48.776 \\ \hline
        270 & 0.689 & 29.859 & 0.277 & 47.623 & 0.328 & 48.776 \\ \hline
        280 & 0.504 & 36.529 & 0.277 & 47.573 & 0.328 & 48.776 \\ \hline
        290 & 0.748 & 24.753 & 0.277 & 47.519 & 0.328 & 48.776 \\ \hline
        300 & 0.664 & 30.485 & 0.277 & 47.460 & 0.328 & 48.776 \\ \hline
        310 & 0.630 & 32.528 & 0.277 & 47.397 & 0.328 & 48.776 \\ \hline
        320 & 0.639 & 31.136 & 0.277 & 47.330 & 0.328 & 48.776 \\ \hline
        330 & 0.588 & 30.805 & 0.277 & 47.260 & 0.328 & 48.776 \\ \hline
        340 & 0.655 & 34.764 & 0.328 & 47.188 & 0.328 & 48.776 \\ \hline
        350 & 0.630 & 28.432 & 0.328 & 47.116 & 0.328 & 48.776 \\ \hline
        360 & 0.681 & 27.222 & 0.328 & 47.047 & 0.328 & 48.776 \\ \hline
        370 & 0.664 & 29.342 & 0.328 & 46.981 & 0.328 & 48.776 \\ \hline
        380 & 0.462 & 44.289 & 0.328 & 46.918 & 0.328 & 48.776 \\ \hline
        390 & 0.605 & 30.552 & 0.328 & 46.861 & 0.328 & 48.776 \\ \hline
        400 & 0.630 & 30.319 & 0.328 & 46.832 & 0.328 & 48.776 \\ \hline
    \end{longtable}
    %    \end{minipage}

    W tym przypadku również modele nie wykazywały żadnej poprawy w procesie uczenia.

    Wartości błędu modelu ze stałym współczynnikiem równym $0.2$ były stałe przez wszyskie epoki równy około 49.
    Podobnie model, któremu zmniejszano współczynnik od $0.2$ do $0.001$.

    Co zaskakujące klasyfikator, który miał wpółczynnik równy $0.1$ bardzo dynamicznie aktualizował wagi, co pozwoliło zmniejszyć błąd nawet do 25 w epoce 290.

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.8]{./img/lr-error-tanh.png}
        \caption{Porównanie wartości błędu modeli $M_{16}$, $M_{17}$ i $M_{18}$}
        %        \label{}
    \end{figure}

    Na wykresie dokładności sytuacja jest podobna.
    W modelu $M_{16}$ wartość dokładności bardzo gwałtownie skacze od około 50\% do 65\%.
    Modele $M_{17}$ i $M_{18}$ nie polepszają się i~po wszystkich epokach mają skuteczność taką jak na początku, czyli prawie 33\%.

    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.8]{./img/lr-accuracy-tanh.png}
        \caption{Porównanie dokładności modeli $M_{16}$, $M_{17}$ i $M_{18}$}
        %        \label{}
    \end{figure}

    \chapter{Podsumowanie}\label{ch:podsumowanie}

    \section{Wnioski z badań}\label{sec:wnioskiZBadań}

    Nie wszystkie modele sieci neuronowej nadają się do rozwiązywania problemu klasyfikacji.
    Niektóre modele radzą sobie dobrze, inne średnio, a jeszcze inne były zupełnie oporne na proces uczenia.
    Badania przeprowadzone na wielu różnych klasyfikatorach pozwalają wyciągnąć ogólne wnioski.

    Można zauważyć, że funkcja aktywacji sigmoid daje lepsze rezultaty niż tanh.
    Każdy z~modeli z funkcją aktywacji sigmoid osiągnął wynik ponad 65\% dokładności po wszystkich epokach uczenia, nawet do 89\%.
    W przypadku tanh kilka najlepszych modeli nieznacznie przekroczyło próg 80\%, ale były też takie, którym nie udało się przeskoczyć 10\%.
    Jest to gorszy wynik niż zgadywanie klasy.

    Porównując wszystkie modele z różnymi architekturami trudno jest wysnuć jakieś ogólne wnioski.
    Choć klasyfikator z 1 warstwą ukrytą często osiągał najlepsze wyniki.
    Na drugim miejscu można by umieścić sieć z dwiema warstami ukrytymi.
    Modele o architekturze 24\_16\_12\_8 i 8 zachowywały się podobnie pomimo różnic w budowie.

    Bardzo wyraźną relację widać pomiędzy liczbą cech a osiągniętą dokładnością.
    Można wywnioskować, że~im więcej cech tym lepiej, przynajmniej dla tego zbioru danych.
    Dla 10 cech najlepsze modele nie przekroczyły skuteczności 85\%, dla 20 cech kilka modeli osiągnęło próg 85\%.
    Modele uczone na 30 cechach klasyfikowały z dokładnością nawet 89\%.

    \section{Dalsze możliwości rozwoju pracy}\label{sec:dalszeMożliwościRozwojuPracy}

    Prace nad sztucznymi sieciami neuronowymi można by dalej rozwijać, na przykład implementując inne funkcje aktywacji.
    Coraz większą popularność wśród badaczy zyskuje funkcja ReLU (Rectified Linear Unit) o wzorze $ReLU(x) = \max(0, x)$ lub Leaky ReLU zadaną wzorem $LReLU(x) = \max(x, 0.001x)$~\cite{cs231_neural_2}.

    W celu unikania przeuczenia pomocne byłoby zastosowanie metody zwanej „dropout”.
    Jest techniką polegającą na losowym odrzuceniu neuronów razem z ich połączeniami z~sieci neuronowej podczas uczenia.
    Chroni to neurony przed zbytnim adaptowaniem się do danych uczących~\cite{dropout}.

    Mając wiedzę medyczną o chorobach można problem klasyfikacji tego zbioru podzielić na 2 etapy.
    Należy pogrupować choroby na 3 grupy w zależności od tego, której części narządów dotyczną.
    Mianowicie podział odbywałby się na schorzenia jelit, organów trawiennych i inne stany ostrego brzucha.
    Pierwszy etap to klasyfikacja z 3 klasami.
    Następnie może nastąpić drugi etap klasyfikacji, gdzie choroby byłyby diagnozowane bardziej szczegółowo.
    Takie podejście wymaga wytrenowania 4 modeli do rozwiązania problemu.

    \listoffigures
    \listoftables

    \bibliography{./bibliography}
    \bibliographystyle{plain}

\end{document}
